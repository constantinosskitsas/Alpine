{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinkhorn import greenkhorn,sinkhorn,sinkhorn_epsilon_scaling,sinkhorn_knopp,sinkhorn_stabilized\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from numpy import linalg as LA\n",
    "from pred import convertToPermHungarian,eucledian_dist,feature_extraction1,feature_extraction,convertToPermHungarian2new\n",
    "import networkx as nx\n",
    "from GradP.gradp import gradPMain\n",
    "import time\n",
    "import os\n",
    "from help_functions import read_graph,read_real_graph, read_list\n",
    "from resultsfolder import generate_new_id,create_new_folder,get_max_previous_id \n",
    "import pandas as pd\n",
    "import scipy \n",
    "from SlotaAlign.SlotaAlign_main import SlotaA\n",
    "from REGAL.regal import RegalATT\n",
    "from HTC.main import HTC_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MKL_NUM_THREADS\"] = \"40\"\n",
    "torch.set_num_threads(40)\n",
    "folderall = 'data3_'\n",
    "experimental_folder=f'./{folderall}/res/'\n",
    "new_id = generate_new_id(get_max_previous_id(experimental_folder))\n",
    "experimental_folder=f'./{folderall}/res/_{new_id}/'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printR(name,forb_norm,accuracy,spec_norm,time_diff,isomorphic=False):\n",
    "    print('---- ',name, '----')\n",
    "    print('----> Forb_norm:', forb_norm)\n",
    "    print('----> Accuracy:', accuracy)\n",
    "    print('----> Time:', time_diff)\n",
    "    print()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PermHungarian(M):\n",
    "\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(M, maximize=True)\n",
    "    return _ ,row_ind,col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_features(F1, F2, A_to_B):\n",
    "    l2_diffs = []\n",
    "    cos_sims = []\n",
    "\n",
    "    for a, b in enumerate(A_to_B):   # <-- works if A_to_B is a list or 1D numpy array\n",
    "        f1 = F1[a]\n",
    "        f2 = F2[b]\n",
    "\n",
    "        # L2 distance\n",
    "        l2 = np.linalg.norm(f1 - f2)\n",
    "        l2_diffs.append(l2)\n",
    "\n",
    "        # Cosine similarity\n",
    "        denom = (np.linalg.norm(f1) * np.linalg.norm(f2) + 1e-9)\n",
    "        cos = np.dot(f1, f2) / denom\n",
    "        cos_sims.append(cos)\n",
    "\n",
    "    print(\"=== Ground Truth Feature Comparison ===\")\n",
    "    print(f\"Average L2 distance: {np.mean(l2_diffs):.6f}\")\n",
    "    print(f\"Median L2 distance: {np.median(l2_diffs):.6f}\")\n",
    "    print(f\"Average cosine similarity: {np.mean(cos_sims):.6f}\")\n",
    "    print(f\"Median cosine similarity: {np.median(cos_sims):.6f}\")\n",
    "    print(f\"Min/Max cosine similarity: {np.min(cos_sims):.6f} / {np.max(cos_sims):.4f}\")\n",
    "    print(f\"distance{np.sum(l2_diffs):.6f}\")\n",
    "    return l2_diffs, cos_sims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_noise_per_row(features, noise_fraction=0.1):\n",
    "    noisy_features = features.copy()\n",
    "    n_rows, n_cols = noisy_features.shape\n",
    "    num_noisy_per_row = int(n_cols * noise_fraction)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        zero_indices = np.random.choice(n_cols, num_noisy_per_row, replace=False)\n",
    "        noisy_features[i, zero_indices] = 0\n",
    "\n",
    "    return noisy_features\n",
    "\n",
    "# Apply to F1 and F2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributed Alpine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic is to scale the attribute information to the structural level\n",
    "\n",
    "            A0 = torch.mean(np.abs(feat1))\n",
    "            S0 = deriv.abs().mean().item() \n",
    "            gamma_a = gamma * S0 / (A0+0.0001 )\n",
    "Experimentaly I noticed that when graphs are sparse the QAP term is ignored\n",
    "so we make it have higher effect in this scenario.    \n",
    "\n",
    "if (avg_degree<3 or avg_degree1<3):\n",
    "        dd=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpine_pp_labels(A,B,feat, K, niter,A1,weight=1):\n",
    "    m = len(A)\n",
    "    n = len(B)\n",
    "    I_p = torch.zeros((m,m+1),dtype = torch.float64)\n",
    "    for i in range(m):\n",
    "        I_p[i,i] = 1\n",
    "    Pi=torch.ones((m+1,n),dtype = torch.float64)\n",
    "    feat1 = torch.tensor(feat, dtype=torch.float64, device=Pi.device)\n",
    "    Pi[:-1,:] *= 1/n\n",
    "    Pi[-1,:] *= (n-m)/n\n",
    "    reg = 1.0\n",
    "    mat_ones = torch.ones((m+1, n), dtype = torch.float64)\n",
    "    ones_ = torch.ones(n, dtype = torch.float64)\n",
    "    ones_augm_ = torch.ones(m+1, dtype = torch.float64)\n",
    "    ones_augm_[-1] = n-m\n",
    "    gamma=1\n",
    "    A0 = torch.mean(np.abs(feat1))\n",
    "    dd=1\n",
    "    degrees = A.sum(dim=1)\n",
    "# Average degree = mean of all degrees\n",
    "    avg_degree = degrees.mean()\n",
    "    degrees1=B.sum(dim=1)\n",
    "    avg_degree1 = degrees1.mean()\n",
    "    if (avg_degree<3 or avg_degree1<3):\n",
    "        dd=2\n",
    "    for i in range(10):\n",
    "        for it in range(1, 11):\n",
    "            deriv=(-4*I_p.T@(A-I_p@Pi@B@Pi.T@I_p.T)@I_p@Pi@B)*dd+i*(mat_ones - 2*Pi)+K\n",
    "            S0 = deriv.abs().mean().item()  # PyTorch version\n",
    "            gamma_a = gamma * S0 / (A0+0.0001 )\n",
    "            deriv = deriv + gamma_a*feat1\n",
    "            q=sinkhorn(ones_augm_, ones_, deriv, reg,method=\"sinkhorn\",maxIter = 500, stopThr = 1e-9) \n",
    "            alpha = (2 / float(2 + it) )    \n",
    "            Pi[:m,:n] = Pi[:m,:n] + alpha * (q[:m,:n] - Pi[:m,:n])\n",
    "    Pi=Pi[:-1]\n",
    "    P2,row_ind,col_ind = convertToPermHungarian(Pi, n, m)\n",
    "    forbnorm = LA.norm(A - I_p[:,:m].T@P2@B@P2.T@I_p[:,:m], 'fro')**2\n",
    "    return Pi, forbnorm,row_ind,col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlpineL(Gq, Gt,f1=None,f2=None, mu=1, niter=10, weight=2):\n",
    "    n1 = Gq.number_of_nodes()\n",
    "    n2 = Gt.number_of_nodes()\n",
    "    n = max(n1, n2)\n",
    "    for node in nx.isolates(Gq):\n",
    "        Gq.add_edge(node, node)\n",
    "    for node in nx.isolates(Gt):\n",
    "        Gt.add_edge(node, node)\n",
    "        \n",
    "    Gq.add_node(n1)\n",
    "    Gq.add_edge(n1,n1)\n",
    "    A = torch.tensor(nx.to_numpy_array(Gq), dtype = torch.float64)\n",
    "    B = torch.tensor(nx.to_numpy_array(Gt), dtype = torch.float64)\n",
    "    feat = eucledian_dist(f1,f2,n)\n",
    "    zeros_row = np.zeros((1, feat.shape[1]))\n",
    "    feat=np.vstack([feat, zeros_row])\n",
    "    \n",
    "# Append it to feat\n",
    "    \n",
    "    #weight=1\n",
    "    if (weight==2):\n",
    "        F1 = feature_extraction1(Gq)\n",
    "        F2 = feature_extraction1(Gt) \n",
    "    else:\n",
    "        F1 = feature_extraction(Gq)\n",
    "        F2 = feature_extraction(Gt)\n",
    "    D = eucledian_dist(F1,F2,n)\n",
    "    D = torch.tensor(D, dtype = torch.float64)\n",
    "    P, forbnorm,row_ind,col_ind = Alpine_pp_labels(A[:n1,:n1], B,feat, mu*D, niter,A)\n",
    "    _, ans=convertToPermHungarian2new(row_ind,col_ind, n1, n2)\n",
    "    list_of_nodes = []\n",
    "    for el in ans: list_of_nodes.append(el[1])\n",
    "    return ans, list_of_nodes, forbnorm    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some specifics of the datasets with the if statements, this is why it is a bit messy.\n",
    "But keep them so it runs correctly.\n",
    "We want the LAP->Attribute to have more effect when label information exist, so we reduce the structural LAP to LAP*0.1 instead of LAP*1 by re-introducing the m term.\n",
    "For the facebook dataset that no labels exist, we keep it to 1.\n",
    "\n",
    "mun=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making ./Data/data/douban/douban_t_edge.txt graph...\n",
      "Done ./Data/data/douban/douban_t_edge.txt ...\n",
      "Graph with 3906 nodes and 7215 edges\n",
      "Making ./Data/data/douban/douban_s_edge.txt graph...\n",
      "Done ./Data/data/douban/douban_s_edge.txt ...\n",
      "True False\n",
      "(3906, 538)\n",
      "G_Q 1511\n",
      "G 7215\n",
      "Alpine\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlpine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     mun\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m--> 108\u001b[0m     _, list_of_nodes, forb_norm \u001b[38;5;241m=\u001b[39m \u001b[43mAlpineL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_Q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mF1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mF2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmun\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(tun[ptun]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradAlignP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mAlpineL\u001b[0;34m(Gq, Gt, f1, f2, mu, niter, weight)\u001b[0m\n\u001b[1;32m     27\u001b[0m D \u001b[38;5;241m=\u001b[39m eucledian_dist(F1,F2,n)\n\u001b[1;32m     28\u001b[0m D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(D, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 29\u001b[0m P, forbnorm,row_ind,col_ind \u001b[38;5;241m=\u001b[39m \u001b[43mAlpine_pp_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m _, ans\u001b[38;5;241m=\u001b[39mconvertToPermHungarian2new(row_ind,col_ind, n1, n2)\n\u001b[1;32m     31\u001b[0m list_of_nodes \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mAlpine_pp_labels\u001b[0;34m(A, B, feat, K, niter, A1, weight)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m         deriv\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mI_p\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m@\u001b[39m(A\u001b[38;5;241m-\u001b[39m\u001b[43mI_p\u001b[49m\u001b[38;5;129;43m@Pi\u001b[39;49m\u001b[38;5;129m@B\u001b[39m\u001b[38;5;129m@Pi\u001b[39m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@I_p\u001b[39m\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;129m@I_p\u001b[39m\u001b[38;5;129m@Pi\u001b[39m\u001b[38;5;129m@B\u001b[39m)\u001b[38;5;241m*\u001b[39mdd\u001b[38;5;241m+\u001b[39mi\u001b[38;5;241m*\u001b[39m(mat_ones \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mPi)\u001b[38;5;241m+\u001b[39mK\n\u001b[1;32m     29\u001b[0m         S0 \u001b[38;5;241m=\u001b[39m deriv\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# PyTorch version\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         gamma_a \u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m S0 \u001b[38;5;241m/\u001b[39m (A0\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.0001\u001b[39m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iters=1\n",
    "tun=[1,6,10,12,14]\n",
    "tuns=[\"Alpine\",\"REGAL\",\"Grad\",\"SlotaA\",\"HTC\"]\n",
    "tun=[1]\n",
    "tuns=[\"Alpine\"]\n",
    "nL=[\"testing\"]\n",
    "foldernames=['douban','allmv_tmdb','acm_dblp','fb_tw','ppi']\n",
    "n_G2 = [1118,5712,9872,1043,1767] #s\n",
    "n_G=[3906,6010,9916,1043,1767] #t\n",
    "gt_size=[1118,5174,6325,1043,1767]\n",
    "foldernames=['douban','allmv_tmdb','acm_dblp','ppi']\n",
    "n_G2 = [1118,5712,9872,1043,1767,2708,1000] #s\n",
    "n_G=   [3906,6010,9916,1043,1767,2708,1003] #t\n",
    "gt_size=[1118,5174,6325,1043,1767,2708,1000]\n",
    "attrN=[True,True,True,True,True]\n",
    "\n",
    "for k in range(0,len(foldernames)):\n",
    "        #G = read_real_graph(n = n_G[k], name_ = f'./raw_data/{foldernames[k]}.txt')\n",
    "        G = read_real_graph(n = n_G[k], name_ = f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_edge.txt')\n",
    "        print(G)\n",
    "        DGS=G.number_of_nodes()\n",
    "    # Get the number of edges\n",
    "        DGES = G.number_of_edges()       \n",
    "        for _ in nL: \n",
    "        #for noiseL in nL: \n",
    "            for ptun in range(len(tun)): \n",
    "                folder = f'./{folderall}/{foldernames[k]}'\n",
    "                os.makedirs(f'{experimental_folder}{foldernames[k]}/{ptun}', exist_ok=True)\n",
    "                folder1=f'./{experimental_folder}/{foldernames[k]}/{ptun}'\n",
    "                file_A_results = open(f'{folder1}/Thesis_results.txt', 'w')\n",
    "                file_A_results.write(f'DGS DGES QGS QGES PGS PGES forb_norm accuracy spec_norm time isomorphic \\n')\n",
    "                F2 = np.loadtxt(f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_feat.txt', dtype=float)  # shape: (n1, k)\n",
    "                F1 = np.loadtxt(f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_feat.txt', dtype=float)  # shape: (n2, k)\n",
    "                #diffs = B_feat[:, None, :] - A_feat[None, :, :]   # shape: (n1, n2, k)\n",
    "                #X = np.abs(diffs).sum(axis=2)  # shape: (n1, n2)\n",
    "                #Feat = np.linalg.norm(diffs, axis=2)  # shape: (n1, n2)\n",
    "                file_real_spectrum = open(f'{folder1}/real_Tspectrum{tuns[ptun]}.txt', 'w')\n",
    "                file_A_spectrum = open(f'{folder1}/A_Tspectrum{tuns[ptun]}.txt', 'w')\n",
    "                #print(f'Size of subgraph: {n_Q}')\n",
    "                \n",
    "                F2=F2\n",
    "                F1=F1\n",
    "                \n",
    "\n",
    "# Split into two arrays\n",
    "                if (foldernames[k]==\"douban\"):\n",
    "                    csv2 = pd.read_csv(f\"./Data/Full-dataset/attribute/{foldernames[k]}attr1.csv\", header=None).iloc[:, 1:].to_numpy()\n",
    "                    csv1 = pd.read_csv(f\"./Data/Full-dataset/attribute/{foldernames[k]}attr2.csv\", header=None).iloc[:, 1:].to_numpy()\n",
    "                    #two versions exists of douban attributes\n",
    "                    #we chose the one which is harder\n",
    "                for iter in range(iters):\n",
    "                    if (foldernames[k]==\"douban\"):\n",
    "                        F2=csv2\n",
    "                        F1=csv1\n",
    "                    #you have to do that because the features have ID making them \n",
    "                    #giving ground truth information\n",
    "                    if (foldernames[k]==\"acm_dblp\"):\n",
    "                        data = np.load(f'JOENA/datasets/ACM-DBLP_0.2.npz')\n",
    "                        F2=data['x2']\n",
    "                        F1=data['x1']\n",
    "\n",
    "                    folder_ = f'{folder}/{iter}'\n",
    "                    folder1_ = f'{folder1}/{iter}'\n",
    "                    os.makedirs(f'{folder1_}', exist_ok=True)\n",
    "                    file_subgraph = f'{folder_}/subgraph.txt'\n",
    "                    file_nodes = f'{folder_}/nodes.txt'\n",
    "                    #Q_real = read_list(file_nodes)\n",
    "                    #G_Q= read_real_graph(n = n_Q, name_ = file_subgraph)\n",
    "                    G_Q = read_real_graph(n = n_G2[k], name_ = f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_edge.txt')\n",
    "                    pairs = []\n",
    "                    with open(f'./Data/data/{foldernames[k]}/{foldernames[k]}_ground_True.txt', \"r\") as f:\n",
    "                        for line in f:\n",
    "                            a, b = line.strip().split()\n",
    "                            pairs.append((int(a), int(b)))\n",
    "                    max_A = n_G2[k]\n",
    "                    max_B = n_G2[k]\n",
    "                    true1=False\n",
    "                    true2=False\n",
    "                    #if douban/dblp/fb_tw no+1 -allmv_tdmbwith +1\n",
    "                    if (foldernames[k]==\"allmv_tmdb\"):\n",
    "                        max_A=max_A+1\n",
    "                        max_B=max_B+1\n",
    "                    A_to_B = [-1] * (max_A)\n",
    "                    for a, b in pairs:\n",
    "                        if (a>=max_A):\n",
    "                            true1=True\n",
    "                        else:\n",
    "                         A_to_B[a] = b\n",
    "# 3️⃣ Build B→A mapping with -1 for missing\n",
    "                    B_to_A = [-1] * (max_B)\n",
    "                    for a, b in pairs:\n",
    "                        if (b>=max_B):\n",
    "                            true2=True\n",
    "                        else:\n",
    "                            B_to_A[b] = a\n",
    "                    print(true1,true2)\n",
    "                    QGS=G_Q.number_of_nodes()\n",
    "                    QGES = G_Q.number_of_edges()\n",
    "                    print(np.shape(F2))\n",
    "                    print(\"G_Q\",G_Q.number_of_edges())\n",
    "                    print(\"G\",G.number_of_edges())\n",
    "                    start = time.time()\n",
    "                    #compare_features(F1,F2,A_to_B)\n",
    "                    #compare_features(F1,F2,B_to_A)\n",
    "                    if(tun[ptun]==1):\n",
    "                        print(\"Alpine\")\n",
    "                        mun=0.1\n",
    "                        _, list_of_nodes, forb_norm = AlpineL(G_Q.copy(), G.copy(),F1,F2,mun,weight=2)\n",
    "                    elif(tun[ptun]==10):\n",
    "                        print(\"GradAlignP\")\n",
    "                        list_of_nodes, forb_norm = gradPMain(G_Q.copy(), G.copy(),F1.copy(),F2.copy())\n",
    "                    elif(tun[ptun]==6):\n",
    "                            print(\"Regal\")\n",
    "                            _, list_of_nodes, forb_norm = RegalATT(G_Q.copy(), G.copy(),F1_n,F2_n)   \n",
    "                    elif(tun[ptun]==12):\n",
    "                        forb_norm=1\n",
    "                        print(\"SlotaAlign\")\n",
    "                        similarity = SlotaA(G_Q.copy(), G.copy(),F1.copy(),F2.copy(),foldernames[k])\n",
    "                        P2, row_ind, col_ind = PermHungarian(similarity)\n",
    "                        #P2, row_ind, col_ind = convertToPermHungarian(similarity, QGS, n_G[k])\n",
    "                        _, ans=convertToPermHungarian2new(row_ind,col_ind, QGS, n_G[k])\n",
    "                        list_of_nodes = []\n",
    "                        for el in ans: list_of_nodes.append(el[1])   \n",
    "                    \n",
    "                    elif tun[ptun] == 14:\n",
    "                        forb_norm=1\n",
    "                        print(\"HTC\")\n",
    "                        #if foldernames[k] in [\"acm_dblp\",\"ppi\",\"cora\"]:\n",
    "                        #    print(\"in\")\n",
    "                        #    data_GT1 = data_GT[:, [1, 0]]  # swap columns\n",
    "                         #   \n",
    "                        #else:\n",
    "                        #    data_GT1=data_GT\n",
    "                        ratio=0 \n",
    "                        data_GT1=None   \n",
    "                        similarity = HTC_main(foldernames[k], ratio, data_GT1, f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_orca.txt', f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_orca.txt', src_laps_name=f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_laps.pth', trg_laps_name=f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_laps.pth')\n",
    "                        if (foldernames[k]==\"douban\" or foldernames[k]==\"allmv_tmdb\"or foldernames[k]==\"foursquare\"or foldernames[k]==\"cora\"or foldernames[k]==\"phone\"):\n",
    "                            similarity=similarity.T\n",
    "                        print('htc shape: ', similarity.shape)\n",
    "                        P2, row_ind, col_ind = PermHungarian(similarity)\n",
    "                        #P2, row_ind, col_ind = convertToPermHungarian(similarity, QGS, n_G[k])\n",
    "                        _, ans=convertToPermHungarian2new(row_ind,col_ind, QGS, n_G[k])\n",
    "                        list_of_nodes = []\n",
    "                        for el in ans: list_of_nodes.append(el[1])\n",
    "                    \n",
    "                    \n",
    "                    else:\n",
    "                        print(\"Error\")\n",
    "                        exit()\n",
    "                    end = time.time()\n",
    "                    subgraph = G.subgraph(list_of_nodes)\n",
    "                    PGS=subgraph.number_of_nodes()\n",
    "                    PGES = subgraph.number_of_edges()\n",
    "                    isomorphic=False\n",
    "                    if(forb_norm==0):\n",
    "                        isomorphic=True\n",
    "                    time_diff = end - start\n",
    "                    file_nodes_pred = open(f'{folder1_}/{tuns[ptun]}.txt','w')\n",
    "                    for node in list_of_nodes: file_nodes_pred.write(f'{node}\\n')\n",
    "                    spec_norm=0\n",
    "                    #accuracy = np.sum(np.array(Q_real)==np.array(list_of_nodes))/len(Q_real)\n",
    "                    accuracy1 = np.sum(np.array(A_to_B)==np.array(list_of_nodes))/gt_size[0]\n",
    "                    accuracy2 = np.sum(np.array(B_to_A)==np.array(list_of_nodes))/gt_size[0]\n",
    "                    print(\"ACC 1 or 2?\",accuracy1,accuracy2)\n",
    "                    print(np.sum(np.array(A_to_B)==np.array(list_of_nodes)))\n",
    "                    accuracy=0\n",
    "                    if ({foldernames[k]}==\"douban\" or{foldernames[k]}==\"allmv_tmdb\" ):\n",
    "                        accuracy=accuracy2\n",
    "                    else:\n",
    "                        accuracy=accuracy1\n",
    "                    with open(\"differences.txt\", \"w\") as f:\n",
    "                        f.write(\"Differences A_to_B:\\n\")\n",
    "                        f.write(\"\\n\\nDifferences B_to_A:\\n\")\n",
    "                        f.write(\"\\n\\nAccuracy A_to_B: {:.4f}\\n\".format(accuracy1))\n",
    "                        f.write(\"Accuracy B_to_A: {:.4f}\\n\".format(accuracy2))\n",
    "                    file_A_results.write(f'{DGS} {DGES} {QGS} {QGES} {PGS} {PGES} {forb_norm} {accuracy1} {accuracy2} {time_diff} {isomorphic}\\n')\n",
    "                    printR(tuns[ptun],forb_norm,accuracy,spec_norm,time_diff,isomorphic)          \n",
    "            print('\\n')\n",
    "        print('\\n\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mypython)",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
