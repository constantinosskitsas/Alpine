{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 100\n",
      "1704\n",
      "Created new folder: ./data3_/res/_1704\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "from pred import eucledian_dist,feature_extraction1,feature_extraction,convertToPermHungarian\n",
    "from pred import convertToPermHungarian2new\n",
    "import ot\n",
    "from numpy import linalg as LA\n",
    "from GradP.gradp import gradPMain\n",
    "import time\n",
    "import os\n",
    "from help_functions import read_graph\n",
    "from help_functions import read_real_graph, read_list\n",
    "from resultsfolder import generate_new_id,create_new_folder,get_max_previous_id \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MKL_NUM_THREADS\"] = \"40\"\n",
    "torch.set_num_threads(40)\n",
    "folderall = 'data3_'\n",
    "experimental_folder=f'./{folderall}/res/'\n",
    "new_id = generate_new_id(get_max_previous_id(experimental_folder))\n",
    "experimental_folder=f'./{folderall}/res/_{new_id}/'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printR(name,forb_norm,accuracy,spec_norm,time_diff,isomorphic=False):\n",
    "    print('---- ',name, '----')\n",
    "    print('----> Forb_norm:', forb_norm)\n",
    "    print('----> Accuracy:', accuracy)\n",
    "    print('----> Time:', time_diff)\n",
    "    print()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def one_hop_cost_neighbors(A, B, gtA, gtB):\n",
    "    nA, nB = A.shape[0], B.shape[0]\n",
    "    device = A.device\n",
    "\n",
    "    # Anchor mapping matrix\n",
    "    M = torch.zeros((nA, nB), dtype=torch.float64, device=device)\n",
    "    M[gtA, gtB] = 1.0\n",
    "\n",
    "    # Count matched anchored neighbors\n",
    "    C = torch.mm(A, torch.mm(M, B.T))  # # of matched anchored neighbors\n",
    "    # Total anchored neighbors for i and j\n",
    "    maskA = torch.zeros(nA, dtype=torch.float64, device=device)\n",
    "    maskA[gtA] = 1.0\n",
    "    degA_gt = A @ maskA.unsqueeze(1)\n",
    "    maskB = torch.zeros(nB, dtype=torch.float64, device=device)\n",
    "    maskB[gtB] = 1.0\n",
    "    degB_gt = B @ maskB.unsqueeze(1)\n",
    "    # Cost = fraction of mismatched anchored neighbors\n",
    "    denom = degA_gt + degB_gt.T\n",
    "    cost = torch.zeros_like(C)\n",
    "    nonzero = denom != 0\n",
    "    w = torch.log1p(denom) / torch.log1p(denom.max())\n",
    "    #cost[nonzero] = (degA_gt + degB_gt.T - 2*C)[nonzero] / denom[nonzero]\n",
    "    cost[nonzero] = w[nonzero] * ((degA_gt + degB_gt.T - 2*C)[nonzero] / denom[nonzero])\n",
    "\n",
    "    # If both nodes have no anchored neighbors, cost = 0\n",
    "    cost[denom == 0] = 0.0\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def two_hop_cost_neighbors(A, B, gtA, gtB):\n",
    "    \"\"\"\n",
    "    Compute 2-hop cost based on ground-truth anchored neighbors.\n",
    "\n",
    "    A, B : adjacency matrices (torch.Tensor)\n",
    "    gtA, gtB : lists/arrays of ground-truth anchors\n",
    "    Returns\n",
    "    -------\n",
    "    cost2H : (nA x nB) tensor of 2-hop costs\n",
    "    \"\"\"\n",
    "    nA, nB = A.shape[0], B.shape[0]\n",
    "    device = A.device\n",
    "\n",
    "    # Compute 2-hop adjacency (binary)\n",
    "    A2 = ((A + A @ A) > 0).double()  # include 1-hop edges as well\n",
    "    B2 = ((B + B @ B) > 0).double()\n",
    "\n",
    "    # Anchor mapping matrix\n",
    "    M = torch.zeros((nA, nB), dtype=torch.float64, device=device)\n",
    "    M[gtA, gtB] = 1.0\n",
    "\n",
    "    # Count matched anchored neighbors (numerator)\n",
    "    C2 = torch.mm(A2, torch.mm(M, B2.T))\n",
    "\n",
    "    # Degrees: total number of anchored neighbors (denominator)\n",
    "    maskA = torch.zeros(nA, dtype=torch.float64, device=device)\n",
    "    maskA[gtA] = 1.0\n",
    "    degA2_gt = A2 @ maskA.unsqueeze(1)\n",
    "\n",
    "    maskB = torch.zeros(nB, dtype=torch.float64, device=device)\n",
    "    maskB[gtB] = 1.0\n",
    "    degB2_gt = B2 @ maskB.unsqueeze(1)\n",
    "\n",
    "    denom2 = degA2_gt + degB2_gt.T\n",
    "\n",
    "    # Compute cost: fraction of mismatched anchored neighbors\n",
    "    cost2 = torch.zeros_like(C2)\n",
    "    nonzero = denom2 != 0\n",
    "    cost2[nonzero] = (degA2_gt + degB2_gt.T - 2*C2)[nonzero] / denom2[nonzero]\n",
    "    w = torch.log1p(denom2) / torch.log1p(denom2.max())\n",
    "    \n",
    "    cost2[nonzero] = w[nonzero] * (degA2_gt + degB2_gt.T - 2*C2)[nonzero] / denom2[nonzero]\n",
    "    \n",
    "    # If both nodes have no anchored neighbors, cost = 0\n",
    "    cost2[denom2 == 0] = 0.0\n",
    "\n",
    "    return cost2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hop_similarity_matrix(A, B, gtA, gtB):\n",
    "    \"\"\"\n",
    "    Vectorized 1-hop similarity based on anchor matches.\n",
    "    A, B: adjacency matrices (torch.Tensor)\n",
    "    gtA, gtB: anchor index lists\n",
    "    \"\"\"\n",
    "    nA, nB = A.shape[0], B.shape[0]\n",
    "    device = A.device\n",
    "\n",
    "    # Anchor matrix M\n",
    "    M = torch.zeros((nA, nB), dtype=torch.float64, device=A.device)\n",
    "    M[gtA, gtB] = 1.0\n",
    "    \n",
    "    # Compute number of matched anchored neighbors\n",
    "    C = torch.mm(A, torch.mm(M, B.T))   # shape (nA, nB)\n",
    "    maskA = torch.zeros(nA, dtype=torch.float64, device=device)\n",
    "    maskA[gtA] = 1.0\n",
    "    degA_gt = A @ maskA.unsqueeze(1)\n",
    "    maskB = torch.zeros(nB, dtype=torch.float64, device=device)\n",
    "    maskB[gtB] = 1.0\n",
    "    degB_gt = B @ maskB.unsqueeze(1)\n",
    "    # Degrees\n",
    "    denom = degA_gt + degB_gt.T  # shape (nA, nB)\n",
    "\n",
    "    \n",
    "    # Similarity matrix\n",
    "    sim = torch.zeros_like(C)\n",
    "    sim[denom == 0] = 1.0\n",
    "    sim[denom != 0] = 2 * C[denom != 0] / denom[denom != 0]\n",
    "    #sim =-2*C\n",
    "    \n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hop_similarity(A, B, gtA, gtB):\n",
    "    \"\"\"\n",
    "    Loop-based 1-hop similarity using ground-truth anchor mapping.\n",
    "\n",
    "    Args:\n",
    "        A, B: adjacency matrices (torch.Tensor)\n",
    "        gtA, gtB: lists of ground-truth anchor indices\n",
    "\n",
    "    Returns:\n",
    "        sim: similarity matrix (nA x nB)\n",
    "    \"\"\"\n",
    "    nA, nB = A.shape[0], B.shape[0]\n",
    "\n",
    "    # Convert to Python ints for safe dict operations\n",
    "    gtA = [int(x) for x in gtA]\n",
    "    gtB = [int(x) for x in gtB]\n",
    "\n",
    "    # Ground-truth anchor mapping\n",
    "    anchor_map = {a: b for a, b in zip(gtA, gtB)}\n",
    "    anchor_values = set(anchor_map.values())\n",
    "\n",
    "    # Precompute neighbors as Python ints\n",
    "    neighA = [list(map(int, torch.nonzero(A[i]).view(-1).tolist())) for i in range(nA)]\n",
    "    neighB = [set(map(int, torch.nonzero(B[j]).view(-1).tolist())) for j in range(nB)]\n",
    "\n",
    "    # Precompute degrees of mappable neighbors\n",
    "    degA_gt = [sum(1 for u in neighA[i] if u in anchor_map) for i in range(nA)]\n",
    "    degB_gt = [sum(1 for v in neighB[j] if v in anchor_values) for j in range(nB)]\n",
    "\n",
    "    # Initialize similarity matrix\n",
    "    sim = torch.zeros((nA, nB), dtype=torch.float64)\n",
    "\n",
    "    # Compute similarity\n",
    "    for i in range(nA):\n",
    "        for j in range(nB):\n",
    "            count = 0\n",
    "            for u in neighA[i]:\n",
    "                if u in anchor_map:\n",
    "                    v = anchor_map[u]\n",
    "                    if v in neighB[j]:\n",
    "                        count += 1\n",
    "\n",
    "            denom = degA_gt[i] + degB_gt[j]\n",
    "            sim[i, j] = 1.0 if denom == 0 else 2 * count / denom\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_hop_similarity_matrix(A, B, gtA, gtB):\n",
    "    \"\"\"\n",
    "    Vectorized 2-hop similarity based on ground-truth anchor matches.\n",
    "    Only neighbors that have a ground-truth correspondence are counted.\n",
    "\n",
    "    Args:\n",
    "        A, B: adjacency matrices (torch.Tensor, shape nA x nA and nB x nB)\n",
    "        gtA, gtB: lists of ground-truth anchor indices\n",
    "\n",
    "    Returns:\n",
    "        sim: similarity matrix (nA x nB)\n",
    "    \"\"\"\n",
    "    nA, nB = A.shape[0], B.shape[0]\n",
    "    device = A.device\n",
    "\n",
    "    # Compute 2-hop adjacency matrices (binary)\n",
    "    A2 = ((A + A @ A) > 0).double()\n",
    "    B2 = ((B + B @ B) > 0).double()\n",
    "    print(A2)\n",
    "    print(B2)\n",
    "    # Anchor mapping matrix\n",
    "    M = torch.zeros((nA, nB), dtype=torch.float64, device=device)\n",
    "    M[gtA, gtB] = 1.0\n",
    "\n",
    "    # Count matched anchored 2-hop neighbors\n",
    "    C = A2 @ M @ B2.T  # shape (nA, nB)\n",
    "\n",
    "    # Precompute degrees of neighbors that can participate in ground-truth matching\n",
    "    maskA = torch.zeros(nA, dtype=torch.float64, device=device)\n",
    "    maskA[gtA] = 1.0\n",
    "    degA_gt = A2 @ maskA.unsqueeze(1)  # shape (nA, 1)\n",
    "\n",
    "    maskB = torch.zeros(nB, dtype=torch.float64, device=device)\n",
    "    maskB[gtB] = 1.0\n",
    "    degB_gt = B2 @ maskB.unsqueeze(1)  # shape (nB, 1)\n",
    "\n",
    "    # Denominator: sum of mappable neighbors\n",
    "    denom = degA_gt + degB_gt.T  # shape (nA, nB)\n",
    "\n",
    "    # Similarity matrix\n",
    "    sim = torch.zeros_like(C)\n",
    "    sim[denom == 0] = 1.0\n",
    "    sim[denom != 0] = 2 * C[denom != 0] / denom[denom != 0]\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_graphs(G, G_Q, anchors_G, anchors_G_Q):\n",
    "    \"\"\"\n",
    "    Create new graphs G1 and G1_Q where edges between aligned anchors\n",
    "    are synchronized across G and G_Q.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : nx.Graph\n",
    "        Original graph G\n",
    "    G_Q : nx.Graph\n",
    "        Original graph G_Q\n",
    "    anchors_G : list[int]\n",
    "        List of nodes in G (anchors)\n",
    "    anchors_G_Q : list[int]\n",
    "        List of corresponding nodes in G_Q (same length as anchors_G)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    G1, G1_Q : nx.Graph\n",
    "        Synchronized graphs\n",
    "    \"\"\"\n",
    "\n",
    "    # Build mapping dicts between anchors\n",
    "    G_to_GQ = dict(zip(anchors_G, anchors_G_Q))\n",
    "    GQ_to_G = dict(zip(anchors_G_Q, anchors_G))\n",
    "\n",
    "    # Start with copies of the original graphs\n",
    "    G1 = G.copy()\n",
    "    G1_Q = G_Q.copy()\n",
    "    counter=0\n",
    "    counter1=0\n",
    "    anchor_nodes = set(G_to_GQ.keys())\n",
    "    anchor_edges = [(u, v) for u, v in G.edges() if u in anchor_nodes and v in anchor_nodes]\n",
    "    print(\"Number of edges among anchors in G:\", len(anchor_edges))\n",
    "\n",
    "    # --- Step 1: Transfer edges from G to G_Q ---\n",
    "    for u, v in G.edges():\n",
    "        if u in G_to_GQ and v in G_to_GQ:\n",
    "            u_q, v_q = G_to_GQ[u], G_to_GQ[v]\n",
    "            if not G1_Q.has_edge(u_q, v_q):\n",
    "                G1_Q.add_edge(u_q, v_q)\n",
    "                counter=counter+1\n",
    "            else:\n",
    "                counter1=counter1+1\n",
    "\n",
    "    # --- Step 2: Transfer edges from G_Q to G ---\n",
    "    for u_q, v_q in G_Q.edges():\n",
    "        if u_q in GQ_to_G and v_q in GQ_to_G:\n",
    "            u, v = GQ_to_G[u_q], GQ_to_G[v_q]\n",
    "            if not G1.has_edge(u, v):\n",
    "                G1.add_edge(u, v)\n",
    "                counter=counter+1\n",
    "            else:\n",
    "                counter1=counter1+1\n",
    "    return G1, G1_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_features(F1, F2, anchors_G, anchors_G_Q, direction=\"F1_to_F2\"):\n",
    "    \"\"\"\n",
    "    Synchronize features along ground truth anchors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    F1 : np.ndarray, shape (n1, k)\n",
    "        Feature matrix of graph G\n",
    "    F2 : np.ndarray, shape (n2, k)\n",
    "        Feature matrix of graph G_Q\n",
    "    anchors_G : list[int]\n",
    "        Anchor node indices in G (for F1)\n",
    "    anchors_G_Q : list[int]\n",
    "        Corresponding anchor node indices in G_Q (for F2)\n",
    "    direction : str\n",
    "        Either \"F1_to_F2\" or \"F2_to_F1\" indicating copy direction\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    F1_new, F2_new : np.ndarray\n",
    "        Feature matrices after synchronization\n",
    "    \"\"\"\n",
    "    # Make copies\n",
    "    F1_new = F1.copy()\n",
    "    F2_new = F2.copy()\n",
    "\n",
    "    if direction == \"F1_to_F2\":\n",
    "        for u, u_q in zip(anchors_G, anchors_G_Q):\n",
    "            F2_new[u_q] = F1_new[u]\n",
    "    elif direction == \"F2_to_F1\":\n",
    "        for u, u_q in zip(anchors_G, anchors_G_Q):\n",
    "            F1_new[u] = F2_new[u_q]\n",
    "    else:\n",
    "        raise ValueError(\"direction must be 'F1_to_F2' or 'F2_to_F1'\")\n",
    "    \n",
    "    return F1_new, F2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_link(seed_list1, seed_list2, G1, G2):\n",
    "    k = 0\n",
    "    for i in range(len(seed_list1) - 1):\n",
    "        for j in range(np.max([1, i + 1]), len(seed_list1)):\n",
    "            if G1.has_edge(seed_list1[i], seed_list1[j]) and not G2.has_edge(seed_list2[i], seed_list2[j]):\n",
    "                G2.add_edges_from([[seed_list2[i], seed_list2[j]]])\n",
    "                k += 1\n",
    "            if not G1.has_edge(seed_list1[i], seed_list1[j]) and G2.has_edge(seed_list2[i], seed_list2[j]):\n",
    "                G1.add_edges_from([[seed_list1[i], seed_list1[j]]])\n",
    "                k += 1\n",
    "    print('Add seed links : {}'.format(k), end='\\t')\n",
    "    return G1, G2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only change from the Attributed version is:\n",
    "We initialize the ground truth pairs in the Partial Permutation matrix with 1\n",
    "We set the distance on the ground truth pairs for LAP_Attribute and LAP_structure to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpine_pp_new_supervised(A, B, feat, K, gtA, gtB, niter, A1, weight=1):\n",
    "    \"\"\"\n",
    "    A: adjacency of graph A (m x m)\n",
    "    B: adjacency of graph B (n x n)\n",
    "    feat: feature/attribute contribution matrix (m x n)\n",
    "    K: structural/regularization matrix (m+1 x n)\n",
    "    gtA, gtB: ground truth anchor lists (matching nodes)\n",
    "    \"\"\"\n",
    "    m = len(A)\n",
    "    n = len(B)\n",
    "    gtA = np.array(gtA, dtype=int)\n",
    "    gtB = np.array(gtB, dtype=int)\n",
    "    # Initialize I_p and Pi\n",
    "    I_p = torch.zeros((m, m + 1), dtype=torch.float64)\n",
    "    for i in range(m):\n",
    "        I_p[i, i] = 1\n",
    "    #SimOH=one_hop_similarity_matrix(A,B,gtA,gtB)\n",
    "    #SimOH1=one_hop_similarity(A,B,gtA,gtB)\n",
    "    #Sim2H=two_hop_similarity_matrix(A,B,gtA,gtB)\n",
    "    #cost1H=1-SimOH\n",
    "    #cost2H=1-Sim2H\n",
    "    #costGT=cost1H#+cost2H\n",
    "    #costGT=costGT*5\n",
    "    costGT=one_hop_cost_neighbors(A,B,gtA,gtB)\n",
    "    costGT=costGT+two_hop_cost_neighbors(A,B,gtA,gtB)\n",
    "    dummy_row = torch.zeros((1, costGT.shape[1]), dtype=costGT.dtype, device=costGT.device)\n",
    "    costGT = torch.cat([costGT, dummy_row], dim=0)\n",
    "    #diff = torch.abs(SimOH - SimOH1)\n",
    "    print(\"Here\")\n",
    "    #if torch.any(diff > 0.001):\n",
    "    #    print(\"Matrices differ\")\n",
    "    Pi = torch.ones((m + 1, n), dtype=torch.float64)\n",
    "    Pi[:-1, :] *= 1 / n\n",
    "    Pi[-1, :] *= (n - m) / n\n",
    "    Pi[-1, :] = 0   \n",
    "\n",
    "    # --- FORCE GROUND TRUTH in Pi at initialization ---\n",
    "    for i, j in zip(gtA, gtB):\n",
    "        Pi[i, :] = 0\n",
    "        Pi[:, j] = 0\n",
    "        Pi[i,j]=1\n",
    "        K[i,j]=0\n",
    "        costGT[i,j]=0\n",
    "    reg = 1.0\n",
    "    mat_ones = torch.ones((m + 1, n), dtype=torch.float64)\n",
    "    ones_ = torch.ones(n, dtype=torch.float64)\n",
    "    ones_augm_ = torch.ones(m + 1, dtype=torch.float64)\n",
    "    ones_augm_[-1] = n - m\n",
    "    gamma = 1\n",
    "    dd=1\n",
    "    degrees = A.sum(dim=1)\n",
    "# Average degree = mean of all degrees\n",
    "    avg_degree = degrees.mean()\n",
    "    degrees1=B.sum(dim=1)\n",
    "    avg_degree1 = degrees1.mean()\n",
    "    if (avg_degree<3 or avg_degree1<3):\n",
    "        dd=2\n",
    "    A0 = np.mean(np.abs(feat))\n",
    "    for outer in range(10):\n",
    "        for it in range(1, 11):\n",
    "            deriv= (-4*I_p.T @ (A - I_p @ Pi @ B @ Pi.T @ I_p.T) @ I_p @ Pi @ B)*dd + outer * (mat_ones - 2 * Pi) + K\n",
    "            S0 = deriv.abs().mean().item()  # magnitude of structural gradient\n",
    "            gamma_a = gamma * S0 / (A0 + 1e-4)\n",
    "            deriv = deriv + gamma_a * (feat)+costGT*0\n",
    "            deriv = deriv +costGT*5\n",
    "\n",
    "            #print(np.max(gamma_a*feat))\n",
    "            q=ot.sinkhorn(ones_augm_, ones_, deriv, 1.0, numItermax = 1500, stopThr = 1e-5)\n",
    "            alpha = 2 / float(2 + it)\n",
    "            Pi[:m, :n] = Pi[:m, :n] + alpha * (q[:m, :n] - Pi[:m, :n])\n",
    "            # --- FORCE GROUND TRUTH AFTER EACH INNER ITERATION ---\n",
    "            for i_gt, j_gt in zip(gtA, gtB):\n",
    "                if(Pi[i_gt, j_gt] == 1):\n",
    "                    continue\n",
    "                Pi[i_gt, :] = 0\n",
    "                Pi[:, j_gt] = 0\n",
    "                Pi[i_gt, j_gt] = 1\n",
    "\n",
    "    Pi = Pi[:-1]\n",
    "\n",
    "    P2, row_ind, col_ind = convertToPermHungarian(Pi, n, m)\n",
    "    forbnorm = LA.norm(A - I_p[:, :m].T @ P2 @ B @ P2.T @ I_p[:, :m], 'fro') ** 2\n",
    "\n",
    "    return Pi, forbnorm, row_ind, col_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpine_supervised(Gq, Gt,f1=None,f2=None,gtGq=None,gtGt=None, mu=1, niter=10, weight=2):\n",
    "    n1 = Gq.number_of_nodes()\n",
    "    n2 = Gt.number_of_nodes()\n",
    "    n = max(n1, n2)\n",
    "    for node in nx.isolates(Gq):\n",
    "        Gq.add_edge(node, node)\n",
    "    for node in nx.isolates(Gt):\n",
    "        Gt.add_edge(node, node)\n",
    "        \n",
    "    Gq.add_node(n1)\n",
    "    Gq.add_edge(n1,n1)\n",
    "    A = torch.tensor(nx.to_numpy_array(Gq), dtype = torch.float64)\n",
    "    B = torch.tensor(nx.to_numpy_array(Gt), dtype = torch.float64)\n",
    "    feat = eucledian_dist(f1,f2,n)\n",
    "    zeros_row = np.zeros((1, feat.shape[1]))\n",
    "    feat=np.vstack([feat, zeros_row])\n",
    "        \n",
    "    if (weight==2):\n",
    "        F1 = feature_extraction1(Gq)\n",
    "        F2 = feature_extraction1(Gt) \n",
    "    else:\n",
    "        F1 = feature_extraction(Gq)\n",
    "        F2 = feature_extraction(Gt)\n",
    "    D = eucledian_dist(F1,F2,n)\n",
    "    D = torch.tensor(D, dtype = torch.float64)\n",
    "    P, forbnorm,row_ind,col_ind = Alpine_pp_new_supervised(A[:n1,:n1], B,feat,mu*D,gtGq,gtGt, niter,A)\n",
    "    _, ans=convertToPermHungarian2new(row_ind,col_ind, n1, n2)\n",
    "    list_of_nodes = []\n",
    "    for el in ans: list_of_nodes.append(el[1])\n",
    "    return ans, list_of_nodes, forbnorm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]], dtype=torch.float64)\n",
      "inside\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[0.6309, 0.0000, 0.6309, 0.0000, 0.6309],\n",
      "        [0.0000, 0.6309, 0.0000, 0.6309, 0.0000],\n",
      "        [0.0000, 0.6309, 0.0000, 0.6309, 0.0000],\n",
      "        [0.6309, 0.0000, 0.6309, 0.0000, 0.6309],\n",
      "        [0.0000, 0.6309, 0.0000, 0.6309, 0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example adjacency matrices\n",
    "A = torch.tensor([\n",
    "    [0, 1, 1, 0,1],\n",
    "    [1, 0, 0, 1,0],\n",
    "    [1, 0, 0, 1,0],\n",
    "    [0, 1, 1, 0,0],\n",
    "    [1, 0, 0, 0,0]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "n = A.shape[0]\n",
    "\n",
    "# Create a random permutation of node indices\n",
    "perm = torch.randperm(n)\n",
    "perm= [1,0,2,3,4]\n",
    "#print(\"Permutation:\", perm.tolist())\n",
    "# Permute the adjacency matrix\n",
    "B = A[perm][:, perm]\n",
    "# Ground-truth anchors: each node i in A maps to perm[i] in B\n",
    "gtA = torch.arange(n)\n",
    "gtB = perm\n",
    "gtA=gtA[:1]\n",
    "gtB=gtB[:1]\n",
    "# Anchors (ground truth)\n",
    "#print(gtA)\n",
    "print(A)\n",
    "print(B)\n",
    "print(\"inside\")\n",
    "#print(one_hop_similarity(A,B,gtA,gtB))\n",
    "print(one_hop_similarity_matrix(A,B,gtA,gtB))\n",
    "print(one_hop_cost_neighbors(A,B,gtA,gtB))\n",
    "#print(two_hop_similarity_matrix(A,B,gtA,gtB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1.]], dtype=torch.float64)\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.6667],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.6667],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.6667],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.6667],\n",
      "        [0.6667, 0.6667, 0.6667, 0.6667, 1.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example adjacency matrices\n",
    "A = torch.tensor([\n",
    "    [0, 1, 1, 0,1],\n",
    "    [1, 0, 0, 1,0],\n",
    "    [1, 0, 0, 1,0],\n",
    "    [0, 1, 1, 0,0],\n",
    "    [1, 0, 0, 0,0]\n",
    "], dtype=torch.float64)\n",
    "\n",
    "n = A.shape[0]\n",
    "\n",
    "# Create a random permutation of node indices\n",
    "perm = torch.randperm(n)\n",
    "perm= [1,0,2,3,4]\n",
    "#print(\"Permutation:\", perm.tolist())\n",
    "# Permute the adjacency matrix\n",
    "B = A[perm][:, perm]\n",
    "# Ground-truth anchors: each node i in A maps to perm[i] in B\n",
    "gtA = torch.arange(n)\n",
    "gtB = perm\n",
    "gtA=gtA[:2]\n",
    "gtB=gtB[:2]\n",
    "gtA=[0,3]\n",
    "gtB=[1,3]\n",
    "# Anchors (ground truth)\n",
    "#print(gtA)\n",
    "#print(A)\n",
    "#print(B)\n",
    "print(\"inside\")\n",
    "#print(one_hop_similarity(A,B,gtA,gtB))\n",
    "#print(one_hop_similarity_matrix(A,B,gtA,gtB))\n",
    "#print(one_hop_cost_neighbors(A,B,gtA,gtB))\n",
    "print(two_hop_similarity_matrix(A,B,gtA,gtB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making ./Data/data/foursquare/foursquare_t_edge.txt graph...\n",
      "Done ./Data/data/foursquare/foursquare_t_edge.txt ...\n",
      "Graph with 5120 nodes and 130575 edges\n",
      "iter 0\n",
      "Making ./Data/data/foursquare/foursquare_s_edge.txt graph...\n",
      "Done ./Data/data/foursquare/foursquare_s_edge.txt ...\n",
      "False False\n",
      "5098  value\n",
      "5281  value\n",
      "(1, 5313)\n",
      "G_Q 54233\n",
      "G 130575\n",
      "Add seed links : 168\tAlpine\n",
      "Here\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5314,5120) (2709,2708) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 138\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(foldernames[k]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfb_tw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m foldernames[k]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfoursquare\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m foldernames[k]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    137\u001b[0m         mun\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 138\u001b[0m     _, list_of_nodes, forb_norm \u001b[38;5;241m=\u001b[39m \u001b[43mAlpine_supervised\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG1_Q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mF1_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43mF2_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43manchors_GQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43manchors_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmun\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m                    \n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(tun[ptun]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradAlignP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[50], line 26\u001b[0m, in \u001b[0;36mAlpine_supervised\u001b[0;34m(Gq, Gt, f1, f2, gtGq, gtGt, mu, niter, weight)\u001b[0m\n\u001b[1;32m     24\u001b[0m D \u001b[38;5;241m=\u001b[39m eucledian_dist(F1,F2,n)\n\u001b[1;32m     25\u001b[0m D \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(D, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m---> 26\u001b[0m P, forbnorm,row_ind,col_ind \u001b[38;5;241m=\u001b[39m \u001b[43mAlpine_pp_new_supervised\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgtGq\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgtGt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mniter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m _, ans\u001b[38;5;241m=\u001b[39mconvertToPermHungarian2new(row_ind,col_ind, n1, n2)\n\u001b[1;32m     28\u001b[0m list_of_nodes \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[63], line 64\u001b[0m, in \u001b[0;36mAlpine_pp_new_supervised\u001b[0;34m(A, B, feat, K, gtA, gtB, niter, A1, weight)\u001b[0m\n\u001b[1;32m     62\u001b[0m S0 \u001b[38;5;241m=\u001b[39m deriv\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# magnitude of structural gradient\u001b[39;00m\n\u001b[1;32m     63\u001b[0m gamma_a \u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m S0 \u001b[38;5;241m/\u001b[39m (A0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m deriv \u001b[38;5;241m=\u001b[39m \u001b[43mderiv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgamma_a\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mcostGT\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     65\u001b[0m deriv \u001b[38;5;241m=\u001b[39m deriv \u001b[38;5;241m+\u001b[39mcostGT\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#print(np.max(gamma_a*feat))\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5314,5120) (2709,2708) "
     ]
    }
   ],
   "source": [
    "iters=1\n",
    "tun=[1,10]\n",
    "tuns=[\"Alpine\",\"Grad\"]\n",
    "tun=[1]\n",
    "tuns=[\"Alpine\"]\n",
    "nL=[\"testing\"]\n",
    "foldernames=['douban','allmv_tmdb','acm_dblp','fb_tw','ppi','cora','foursquare','phone']\n",
    "n_G2 = [1118,5712,9872,1043,1767,2708,5313,1000] #s\n",
    "n_G=[3906,6010,9916,1043,1767,2708,5120,1003] #t\n",
    "gt_size=[1118,5174,6325,1043,1767,2708,1609,1000]\n",
    "\n",
    "foldernames=['foursquare']\n",
    "n_G2 = [5313] #s\n",
    "n_G=[5120] #t\n",
    "gt_size=[1609]\n",
    "\n",
    "for k in range(0,len(foldernames)):\n",
    "        #G = read_real_graph(n = n_G[k], name_ = f'./raw_data/{foldernames[k]}.txt')\n",
    "        G = read_real_graph(n = n_G[k], name_ = f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_edge.txt')\n",
    "        print(G)\n",
    "        DGS=G.number_of_nodes()\n",
    "    # Get the number of edges\n",
    "        DGES = G.number_of_edges()       \n",
    "        for _ in nL: \n",
    "        #for noiseL in nL: \n",
    "            for ptun in range(len(tun)): \n",
    "                folder = f'./{folderall}/{foldernames[k]}'\n",
    "                os.makedirs(f'{experimental_folder}{foldernames[k]}/{ptun}', exist_ok=True)\n",
    "                folder1=f'./{experimental_folder}/{foldernames[k]}/{ptun}'\n",
    "                file_A_results = open(f'{folder1}/Thesis_results.txt', 'w')\n",
    "                file_A_results.write(f'DGS DGES QGS QGES PGS PGES forb_norm accuracy spec_norm time isomorphic \\n')\n",
    "                \n",
    "                if (foldernames[k]!=\"foursquare\" and foldernames[k]!=\"phone\"):\n",
    "                    F2 = np.loadtxt(f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_feat.txt', dtype=float)  # shape: (n1, k)\n",
    "                    F1 = np.loadtxt(f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_feat.txt', dtype=float)  # shape: (n2, k)\n",
    "                else:\n",
    "                    F1=np.zeros((1,n_G[k]))\n",
    "                    F2=np.zeros((1,n_G2[k]))\n",
    "\n",
    "                #diffs = B_feat[:, None, :] - A_feat[None, :, :]   # shape: (n1, n2, k)\n",
    "                #X = np.abs(diffs).sum(axis=2)  # shape: (n1, n2)\n",
    "                #Feat = np.linalg.norm(diffs, axis=2)  # shape: (n1, n2)\n",
    "                file_real_spectrum = open(f'{folder1}/real_Tspectrum{tuns[ptun]}.txt', 'w')\n",
    "                file_A_spectrum = open(f'{folder1}/A_Tspectrum{tuns[ptun]}.txt', 'w')\n",
    "                F2=F2\n",
    "                F1=F1\n",
    "                \n",
    "\n",
    "# Split into two arrays\n",
    "                if (foldernames[k]==\"douban\"):\n",
    "                    csv2 = pd.read_csv(f\"./Data/Full-dataset/attribute/{foldernames[k]}attr1.csv\", header=None).iloc[:, 1:].to_numpy()\n",
    "                    csv1 = pd.read_csv(f\"./Data/Full-dataset/attribute/{foldernames[k]}attr2.csv\", header=None).iloc[:, 1:].to_numpy()\n",
    "                \n",
    "\n",
    "                for iter in range(iters):\n",
    "                    print(\"iter\",iter)\n",
    "                    if (foldernames[k]==\"douban\"):\n",
    "                        F2=csv2\n",
    "                        F1=csv1\n",
    "                    #you have to do that because the features have ID making them \n",
    "                    #giving ground truth information\n",
    "                    if (foldernames[k]==\"fb_tw\"):\n",
    "                        F2=F2*0\n",
    "                        F1=F1*0\n",
    "                        \n",
    "                    data_GT = np.loadtxt(f\"./Data/data/{foldernames[k]}/{foldernames[k]}_ground_True_0.1_{iter}.txt\", dtype=int)\n",
    "                    #F2=csv2\n",
    "                    #F1=csv1\n",
    "                    folder_ = f'{folder}/{iter}'\n",
    "                    folder1_ = f'{folder1}/{iter}'\n",
    "                    os.makedirs(f'{folder1_}', exist_ok=True)\n",
    "                    file_subgraph = f'{folder_}/subgraph.txt'\n",
    "                    file_nodes = f'{folder_}/nodes.txt'\n",
    "                    #Q_real = read_list(file_nodes)\n",
    "                    #G_Q= read_real_graph(n = n_Q, name_ = file_subgraph)\n",
    "                    G_Q = read_real_graph(n = n_G2[k], name_ = f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_edge.txt')\n",
    "                    pairs = []\n",
    "                    with open(f'./Data/data/{foldernames[k]}/{foldernames[k]}_ground_True.txt', \"r\") as f:\n",
    "                        for line in f:\n",
    "                            a, b = line.strip().split()\n",
    "                            pairs.append((int(a), int(b)))\n",
    "\n",
    "# ✅ Find the max node ID in each graph\n",
    "                    max_A = n_G2[k]#max(a for a, b in pairs)\n",
    "                    max_B = n_G2[k]#max(b for a, b in pairs)\n",
    "\n",
    "# 2️⃣ Build A→B mapping with -1 for missing\n",
    "                    true1=False\n",
    "                    true2=False\n",
    "                    #if douban/dblp/fb_tw no+1 -allmv_tmdbwith +1\n",
    "                    if (foldernames[k]==\"allmv_tmdb\"):\n",
    "                        max_A=max_A+1\n",
    "                        max_B=max_B+1\n",
    "                    A_to_B = [-1] * (max_A)\n",
    "                    for a, b in pairs:\n",
    "                        if (a>=max_A):\n",
    "                            true1=True\n",
    "                        else:\n",
    "                         A_to_B[a] = b\n",
    "\n",
    "# 3️⃣ Build B→A mapping with -1 for missing\n",
    "                    B_to_A = [-1] * (max_B)\n",
    "                    for a, b in pairs:\n",
    "                        if (b>=max_B):\n",
    "                            true2=True\n",
    "                        else:\n",
    "                            B_to_A[b] = a\n",
    "                    print(true1,true2)\n",
    "                    #A = nx.adjacency_matrix(G_Q).todense()\n",
    "                    QGS=G_Q.number_of_nodes()\n",
    "                    QGES = G_Q.number_of_edges()\n",
    "                    anchors_G = data_GT[:, 1].tolist()\n",
    "                    anchors_GQ = data_GT[:, 0].tolist()\n",
    "                    #G1,G1_Q=synchronize_graphs(G,G_Q,anchors_G,anchors_GQ)\n",
    "                    print(max(anchors_G),\" value\")\n",
    "                    print(max(anchors_GQ),\" value\")\n",
    "                    print(np.shape(F2))\n",
    "                    print(\"G_Q\",G_Q.number_of_edges())\n",
    "                    print(\"G\",G.number_of_edges())\n",
    "                    if (foldernames[k]==\"douban\" or foldernames[k]==\"allmv_tmdb\"):\n",
    "                        anchors_G = data_GT[:, 0].tolist()\n",
    "                        anchors_GQ = data_GT[:, 1].tolist()\n",
    "                    \n",
    "                    G1,G1_Q=seed_link(anchors_G,anchors_GQ,G,G_Q)\n",
    "                    if (foldernames[k]!=\"foursquare\" and foldernames[k]!=\"phone\"):\n",
    "\n",
    "                        F2_n,F1_n=synchronize_features(F2,F1,anchors_G,anchors_GQ)\n",
    "                    #F2_n*=0\n",
    "                    #F1_n*=0\n",
    "                    start = time.time()\n",
    "                    #compare_features(F1,F2,A_to_B)\n",
    "                    #compare_features(F1,F2,B_to_A)\n",
    "                    if(tun[ptun]==1):\n",
    "                        print(\"Alpine\")\n",
    "                        mun=0.1\n",
    "                        #if(foldernames[k]==\"fb_tw\" or foldernames[k]==\"foursquare\" or foldernames[k]==\"phone\"):\n",
    "                        if(foldernames[k]==\"fb_tw\" or foldernames[k]==\"phone\"):\n",
    "                            mun=1\n",
    "                        _, list_of_nodes, forb_norm = Alpine_supervised(G1_Q.copy(), G1.copy(),F1_n,F2_n,anchors_GQ,anchors_G,mun,weight=2)                    \n",
    "                    elif(tun[ptun]==10):\n",
    "                        print(\"GradAlignP\")\n",
    "                        list_of_nodes, forb_norm = gradPMain(G_Q.copy(), G.copy(),F1.copy(),F2.copy(),anchors_GQ=anchors_GQ,anchors_G=anchors_G)\n",
    "                    else:\n",
    "                        print(\"Error\")\n",
    "                        exit()\n",
    "                    end = time.time()\n",
    "                    subgraph = G.subgraph(list_of_nodes)\n",
    "                    PGS=subgraph.number_of_nodes()\n",
    "                    PGES = subgraph.number_of_edges()\n",
    "                    isomorphic=False\n",
    "                    if(forb_norm==0):\n",
    "                        isomorphic=True\n",
    "                    time_diff = end - start\n",
    "                    file_nodes_pred = open(f'{folder1_}/{tuns[ptun]}.txt','w')\n",
    "                    for node in list_of_nodes: file_nodes_pred.write(f'{node}\\n')\n",
    "                    spec_norm=0\n",
    "                    #accuracy = np.sum(np.array(Q_real)==np.array(list_of_nodes))/len(Q_real)\n",
    "                    accuracy1 = np.sum(np.array(A_to_B)==np.array(list_of_nodes))/gt_size[0]\n",
    "                    accuracy2 = np.sum(np.array(B_to_A)==np.array(list_of_nodes))/gt_size[0]\n",
    "                    accuracy=0\n",
    "                    if ({foldernames[k]}==\"douban\" or{foldernames[k]}==\"allmv_tmdb\" ):\n",
    "                        accuracy=accuracy2\n",
    "                    else:\n",
    "                        accuracy=accuracy1\n",
    "                    print(\"ACC 1 or 2?\",accuracy1,accuracy2)\n",
    "                    print(np.sum(np.array(A_to_B)==np.array(list_of_nodes)))\n",
    "                    with open(\"differences.txt\", \"w\") as f:\n",
    "                        f.write(\"Differences A_to_B:\\n\")\n",
    "                        f.write(\"\\n\\nDifferences B_to_A:\\n\")\n",
    "                        f.write(\"\\n\\nAccuracy A_to_B: {:.4f}\\n\".format(accuracy1))\n",
    "                        f.write(\"Accuracy B_to_A: {:.4f}\\n\".format(accuracy2))\n",
    "                    file_A_results.write(f'{DGS} {DGES} {QGS} {QGES} {PGS} {PGES} {forb_norm} {accuracy1} {accuracy2} {time_diff} {isomorphic}\\n')\n",
    "                    printR(tuns[ptun],forb_norm,accuracy,spec_norm,time_diff,isomorphic)          \n",
    "            print('\\n')\n",
    "        print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'H', 'email', 'gnd', 'phone'])\n",
      "[[  0  18]\n",
      " [  0  21]\n",
      " [  0  30]\n",
      " ...\n",
      " [999 995]\n",
      " [999 997]\n",
      " [999 998]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "data = loadmat(f'Data/data/phone-email.mat')\n",
    "print(data.keys())\n",
    "A1 = data['phone']              # adjacency of graph 1\n",
    "A2 = data['email']              # adjacency of graph 2\n",
    "#X1 = data['cora1_node_feat']    # node features of graph 1\n",
    "#X2 = data['cora2_node_feat']    # node features of graph 2\n",
    "gnd = data['gnd'].squeeze()     # ground truth correspondence\n",
    "#edge_list1 = np.array(np.nonzero(A1)).T  # shape [E1, 2]\n",
    "#edge_list2 = np.array(np.nonzero(A2)).T  # shape [E2, 2]\n",
    "A1 = sparse.csr_matrix(A1)\n",
    "A2 = sparse.csr_matrix(A2)\n",
    "\n",
    "# Get edge lists (i, j) pairs\n",
    "edge_list1 = np.vstack(A1.nonzero()).T\n",
    "edge_list2 = np.vstack(A2.nonzero()).T\n",
    "if gnd.min() == 1:\n",
    "    gnd = gnd - 1\n",
    "# Convert to 0-based indices if needed\n",
    "if edge_list1.min() == 1:\n",
    "    edge_list1 -= 1\n",
    "if edge_list2.min() == 1:\n",
    "    edge_list2 -= 1\n",
    "print(edge_list1)\n",
    "#edge_list1 = np.array(np.triu(A1, k=1).nonzero()).T\n",
    "#edge_list2 = np.array(np.triu(A2, k=1).nonzero()).T\n",
    "#np.savetxt(f'Data/data/phone/phone_s_edge.txt', edge_list1, fmt='%d')\n",
    "#np.savetxt('Data/data/phone/phone_t_edge.txt', edge_list2, fmt='%d')\n",
    "#np.savetxt('Data/data/cora/cora_s_feat.txt', X1, fmt='%.2f')\n",
    "#np.savetxt('Data/data/cora/cora_t_feat.txt', X2, fmt='%.2f')\n",
    "#np.savetxt('Data/data/phone/phone.txt', gnd, fmt='%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tri() missing 1 required positional argument: 'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gnd\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     16\u001b[0m     gnd \u001b[38;5;241m=\u001b[39m gnd \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 18\u001b[0m edge_list1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnonzero())\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     19\u001b[0m edge_list2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39mtriu(A2, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnonzero())\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Ensure 0-based (some datasets may still store 1-based edges)\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mtriu\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/twodim_base.py:494\u001b[0m, in \u001b[0;36mtriu\u001b[0;34m(m, k)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03mUpper triangle of an array.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    493\u001b[0m m \u001b[38;5;241m=\u001b[39m asanyarray(m)\n\u001b[0;32m--> 494\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mtri\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m where(mask, zeros(\u001b[38;5;241m1\u001b[39m, m\u001b[38;5;241m.\u001b[39mdtype), m)\n",
      "\u001b[0;31mTypeError\u001b[0m: tri() missing 1 required positional argument: 'N'"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# --- Load the .mat file ---\n",
    "data = loadmat(f'Data/data/foursquare-twitter.mat')\n",
    "\n",
    "# --- Extract data ---\n",
    "A1 = data['foursquare']\n",
    "A2 = data['twitter']\n",
    "#X1 = data['cora1_node_feat']\n",
    "#X2 = data['cora2_node_feat']\n",
    "gnd = data['gnd'].squeeze()\n",
    "\n",
    "# --- Convert to 0-based indexing (MATLAB → Python) ---\n",
    "if gnd.min() == 1:\n",
    "    gnd = gnd - 1\n",
    "\n",
    "edge_list1 = np.array(np.triu(A1, 1).nonzero()).T\n",
    "edge_list2 = np.array(np.triu(A2, 1).nonzero()).T\n",
    "\n",
    "# Ensure 0-based (some datasets may still store 1-based edges)\n",
    "if edge_list1.min() == 1:\n",
    "    edge_list1 -= 1\n",
    "if edge_list2.min() == 1:   \n",
    "    edge_list2 -= 1\n",
    "edge_list1 = edge_list1.astype(int)\n",
    "edge_list2 = edge_list2.astype(int)\n",
    "\n",
    "# --- Save everything as .txt files ---\n",
    "#np.savetxt(F'Data/data/foursquare/foursquare_s_edge.txt', edge_list1, fmt='%d')\n",
    "#np.savetxt('Data/data/foursquare/foursquare_t_edge.txt', edge_list2, fmt='%d')\n",
    "#np.savetxt('Data/data/cora/cora_s_feat.txt', X1, fmt='%.2f')\n",
    "#np.savetxt('Data/data/cora/cora_t_feat.txt', X2, fmt='%.2f')\n",
    "#np.savetxt('Data/data/foursquare/foursquare_ground_True.txt', gnd, fmt='%d')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mypython)",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
