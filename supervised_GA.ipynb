{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "from pred import eucledian_dist,feature_extraction1,feature_extraction,convertToPermHungarian\n",
    "from pred import convertToPermHungarian2new\n",
    "import ot\n",
    "from numpy import linalg as LA\n",
    "from GradP.gradp import gradPMain\n",
    "import time\n",
    "import os\n",
    "from help_functions import read_graph\n",
    "from help_functions import read_real_graph, read_list\n",
    "from resultsfolder import generate_new_id,create_new_folder,get_max_previous_id \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MKL_NUM_THREADS\"] = \"40\"\n",
    "torch.set_num_threads(40)\n",
    "folderall = 'data3_'\n",
    "experimental_folder=f'./{folderall}/res/'\n",
    "new_id = generate_new_id(get_max_previous_id(experimental_folder))\n",
    "experimental_folder=f'./{folderall}/res/_{new_id}/'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printR(name,forb_norm,accuracy,spec_norm,time_diff,isomorphic=False):\n",
    "    print('---- ',name, '----')\n",
    "    print('----> Forb_norm:', forb_norm)\n",
    "    print('----> Accuracy:', accuracy)\n",
    "    print('----> Time:', time_diff)\n",
    "    print()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_graphs(G, G_Q, anchors_G, anchors_G_Q):\n",
    "    \"\"\"\n",
    "    Create new graphs G1 and G1_Q where edges between aligned anchors\n",
    "    are synchronized across G and G_Q.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : nx.Graph\n",
    "        Original graph G\n",
    "    G_Q : nx.Graph\n",
    "        Original graph G_Q\n",
    "    anchors_G : list[int]\n",
    "        List of nodes in G (anchors)\n",
    "    anchors_G_Q : list[int]\n",
    "        List of corresponding nodes in G_Q (same length as anchors_G)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    G1, G1_Q : nx.Graph\n",
    "        Synchronized graphs\n",
    "    \"\"\"\n",
    "\n",
    "    # Build mapping dicts between anchors\n",
    "    G_to_GQ = dict(zip(anchors_G, anchors_G_Q))\n",
    "    GQ_to_G = dict(zip(anchors_G_Q, anchors_G))\n",
    "\n",
    "    # Start with copies of the original graphs\n",
    "    G1 = G.copy()\n",
    "    G1_Q = G_Q.copy()\n",
    "    counter=0\n",
    "    counter1=0\n",
    "    anchor_nodes = set(G_to_GQ.keys())\n",
    "    anchor_edges = [(u, v) for u, v in G.edges() if u in anchor_nodes and v in anchor_nodes]\n",
    "    print(\"Number of edges among anchors in G:\", len(anchor_edges))\n",
    "\n",
    "    # --- Step 1: Transfer edges from G to G_Q ---\n",
    "    for u, v in G.edges():\n",
    "        if u in G_to_GQ and v in G_to_GQ:\n",
    "            u_q, v_q = G_to_GQ[u], G_to_GQ[v]\n",
    "            if not G1_Q.has_edge(u_q, v_q):\n",
    "                G1_Q.add_edge(u_q, v_q)\n",
    "                counter=counter+1\n",
    "            else:\n",
    "                counter1=counter1+1\n",
    "\n",
    "    # --- Step 2: Transfer edges from G_Q to G ---\n",
    "    for u_q, v_q in G_Q.edges():\n",
    "        if u_q in GQ_to_G and v_q in GQ_to_G:\n",
    "            u, v = GQ_to_G[u_q], GQ_to_G[v_q]\n",
    "            if not G1.has_edge(u, v):\n",
    "                G1.add_edge(u, v)\n",
    "                counter=counter+1\n",
    "            else:\n",
    "                counter1=counter1+1\n",
    "    print(counter)\n",
    "    print(counter1)\n",
    "    return G1, G1_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_features(F1, F2, anchors_G, anchors_G_Q, direction=\"F1_to_F2\"):\n",
    "    \"\"\"\n",
    "    Synchronize features along ground truth anchors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    F1 : np.ndarray, shape (n1, k)\n",
    "        Feature matrix of graph G\n",
    "    F2 : np.ndarray, shape (n2, k)\n",
    "        Feature matrix of graph G_Q\n",
    "    anchors_G : list[int]\n",
    "        Anchor node indices in G (for F1)\n",
    "    anchors_G_Q : list[int]\n",
    "        Corresponding anchor node indices in G_Q (for F2)\n",
    "    direction : str\n",
    "        Either \"F1_to_F2\" or \"F2_to_F1\" indicating copy direction\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    F1_new, F2_new : np.ndarray\n",
    "        Feature matrices after synchronization\n",
    "    \"\"\"\n",
    "    # Make copies\n",
    "    F1_new = F1.copy()\n",
    "    F2_new = F2.copy()\n",
    "\n",
    "    if direction == \"F1_to_F2\":\n",
    "        for u, u_q in zip(anchors_G, anchors_G_Q):\n",
    "            F2_new[u_q] = F1_new[u]\n",
    "    elif direction == \"F2_to_F1\":\n",
    "        for u, u_q in zip(anchors_G, anchors_G_Q):\n",
    "            F1_new[u] = F2_new[u_q]\n",
    "    else:\n",
    "        raise ValueError(\"direction must be 'F1_to_F2' or 'F2_to_F1'\")\n",
    "    \n",
    "    return F1_new, F2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_link(seed_list1, seed_list2, G1, G2):\n",
    "    k = 0\n",
    "    for i in range(len(seed_list1) - 1):\n",
    "        for j in range(np.max([1, i + 1]), len(seed_list1)):\n",
    "            if G1.has_edge(seed_list1[i], seed_list1[j]) and not G2.has_edge(seed_list2[i], seed_list2[j]):\n",
    "                G2.add_edges_from([[seed_list2[i], seed_list2[j]]])\n",
    "                k += 1\n",
    "            if not G1.has_edge(seed_list1[i], seed_list1[j]) and G2.has_edge(seed_list2[i], seed_list2[j]):\n",
    "                G1.add_edges_from([[seed_list1[i], seed_list1[j]]])\n",
    "                k += 1\n",
    "    print('Add seed links : {}'.format(k), end='\\t')\n",
    "    return G1, G2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only change from the Attributed version is:\n",
    "We initialize the ground truth pairs in the Partial Permutation matrix with 1\n",
    "We set the distance on the ground truth pairs for LAP_Attribute and LAP_structure to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpine_pp_new_supervised(A, B, feat, K, gtA, gtB, niter, A1, weight=1):\n",
    "    \"\"\"\n",
    "    A: adjacency of graph A (m x m)\n",
    "    B: adjacency of graph B (n x n)\n",
    "    feat: feature/attribute contribution matrix (m x n)\n",
    "    K: structural/regularization matrix (m+1 x n)\n",
    "    gtA, gtB: ground truth anchor lists (matching nodes)\n",
    "    \"\"\"\n",
    "    m = len(A)\n",
    "    n = len(B)\n",
    "    gtA = np.array(gtA, dtype=int)\n",
    "    gtB = np.array(gtB, dtype=int)\n",
    "    # Initialize I_p and Pi\n",
    "    I_p = torch.zeros((m, m + 1), dtype=torch.float64)\n",
    "    for i in range(m):\n",
    "        I_p[i, i] = 1\n",
    "\n",
    "    Pi = torch.ones((m + 1, n), dtype=torch.float64)\n",
    "    Pi[:-1, :] *= 1 / n\n",
    "    Pi[-1, :] *= (n - m) / n\n",
    "    Pi[-1, :] = 0   \n",
    "\n",
    "    # --- FORCE GROUND TRUTH in Pi at initialization ---\n",
    "    for i, j in zip(gtA, gtB):\n",
    "        Pi[i, :] = 0\n",
    "        Pi[:, j] = 0\n",
    "        Pi[i,j]=1\n",
    "        K[i,j]=0\n",
    "    reg = 1.0\n",
    "    mat_ones = torch.ones((m + 1, n), dtype=torch.float64)\n",
    "    ones_ = torch.ones(n, dtype=torch.float64)\n",
    "    ones_augm_ = torch.ones(m + 1, dtype=torch.float64)\n",
    "    ones_augm_[-1] = n - m\n",
    "    gamma = 1\n",
    "    dd=1\n",
    "    degrees = A.sum(dim=1)\n",
    "# Average degree = mean of all degrees\n",
    "    avg_degree = degrees.mean()\n",
    "    degrees1=B.sum(dim=1)\n",
    "    avg_degree1 = degrees1.mean()\n",
    "    if (avg_degree<3 or avg_degree1<3):\n",
    "        dd=2\n",
    "    A0 = np.mean(np.abs(feat))\n",
    "    for outer in range(10):\n",
    "        for it in range(1, 11):\n",
    "            deriv= (-4*I_p.T @ (A - I_p @ Pi @ B @ Pi.T @ I_p.T) @ I_p @ Pi @ B)*dd + outer * (mat_ones - 2 * Pi) + K\n",
    "            S0 = deriv.abs().mean().item()  # magnitude of structural gradient\n",
    "            gamma_a = gamma * S0 / (A0 + 1e-4)\n",
    "            deriv = deriv + gamma_a * feat\n",
    "            q=ot.sinkhorn(ones_augm_, ones_, deriv, 1.0, numItermax = 500, stopThr = 1e-5)\n",
    "            alpha = 2 / float(2 + it)\n",
    "            Pi[:m, :n] = Pi[:m, :n] + alpha * (q[:m, :n] - Pi[:m, :n])\n",
    "            # --- FORCE GROUND TRUTH AFTER EACH INNER ITERATION ---\n",
    "            for i_gt, j_gt in zip(gtA, gtB):\n",
    "                if(Pi[i_gt, j_gt] == 1):\n",
    "                    continue\n",
    "                Pi[i_gt, :] = 0\n",
    "                Pi[:, j_gt] = 0\n",
    "                Pi[i_gt, j_gt] = 1\n",
    "\n",
    "    Pi = Pi[:-1]\n",
    "\n",
    "    P2, row_ind, col_ind = convertToPermHungarian(Pi, n, m)\n",
    "    forbnorm = LA.norm(A - I_p[:, :m].T @ P2 @ B @ P2.T @ I_p[:, :m], 'fro') ** 2\n",
    "\n",
    "    return Pi, forbnorm, row_ind, col_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpine_supervised(Gq, Gt,f1=None,f2=None,gtGq=None,gtGt=None, mu=1, niter=10, weight=2):\n",
    "    n1 = Gq.number_of_nodes()\n",
    "    n2 = Gt.number_of_nodes()\n",
    "    n = max(n1, n2)\n",
    "    for node in nx.isolates(Gq):\n",
    "        Gq.add_edge(node, node)\n",
    "    for node in nx.isolates(Gt):\n",
    "        Gt.add_edge(node, node)\n",
    "        \n",
    "    Gq.add_node(n1)\n",
    "    Gq.add_edge(n1,n1)\n",
    "    A = torch.tensor(nx.to_numpy_array(Gq), dtype = torch.float64)\n",
    "    B = torch.tensor(nx.to_numpy_array(Gt), dtype = torch.float64)\n",
    "    feat = eucledian_dist(f1,f2,n)\n",
    "    zeros_row = np.zeros((1, feat.shape[1]))\n",
    "    feat=np.vstack([feat, zeros_row])\n",
    "        \n",
    "    if (weight==2):\n",
    "        F1 = feature_extraction1(Gq)\n",
    "        F2 = feature_extraction1(Gt) \n",
    "    else:\n",
    "        F1 = feature_extraction(Gq)\n",
    "        F2 = feature_extraction(Gt)\n",
    "    D = eucledian_dist(F1,F2,n)\n",
    "    D = torch.tensor(D, dtype = torch.float64)\n",
    "    P, forbnorm,row_ind,col_ind = Alpine_pp_new_supervised(A[:n1,:n1], B,feat,mu*D,gtGq,gtGt, niter,A)\n",
    "    _, ans=convertToPermHungarian2new(row_ind,col_ind, n1, n2)\n",
    "    list_of_nodes = []\n",
    "    for el in ans: list_of_nodes.append(el[1])\n",
    "    return ans, list_of_nodes, forbnorm    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making ./Data/data/acm_dblp/acm_dblp_t_edge.txt graph...\n",
      "Done ./Data/data/acm_dblp/acm_dblp_t_edge.txt ...\n",
      "Graph with 9916 nodes and 44808 edges\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "./Data/data/acm_dblp/acm_dblp_ground_True_0.1_0.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     F2\u001b[38;5;241m=\u001b[39mF2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     54\u001b[0m     F1\u001b[38;5;241m=\u001b[39mF1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 55\u001b[0m data_GT \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Data/data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfoldernames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfoldernames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ground_True_0.1_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43miter\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#F2=csv2\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#F1=csv1\u001b[39;00m\n\u001b[1;32m     58\u001b[0m folder_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:1065\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_string_like(fname):\n\u001b[0;32m-> 1065\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1066\u001b[0m     fencoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1067\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(fh)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py:194\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py:531\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    529\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m path)\n",
      "\u001b[0;31mOSError\u001b[0m: ./Data/data/acm_dblp/acm_dblp_ground_True_0.1_0.txt not found."
     ]
    }
   ],
   "source": [
    "iters=1\n",
    "tun=[1,10]\n",
    "tuns=[\"Alpine\",\"Grad\"]\n",
    "nL=[\"testing\"]\n",
    "foldernames=['douban','allmv_tmdb','acm_dblp','fb_tw','ppi']\n",
    "n_G2 = [1118,5712,9872,1043,1767] #s\n",
    "n_G=[3906,6010,9916,1043,1767] #t\n",
    "gt_size=[1118,5174,6325,1043,1767]\n",
    "foldernames=['acm_dblp']\n",
    "n_G2 = [9872] #s\n",
    "n_G=[9916] #t\n",
    "gt_size=[6325]\n",
    "\n",
    "for k in range(0,len(foldernames)):\n",
    "        #G = read_real_graph(n = n_G[k], name_ = f'./raw_data/{foldernames[k]}.txt')\n",
    "        G = read_real_graph(n = n_G[k], name_ = f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_edge.txt')\n",
    "        print(G)\n",
    "        DGS=G.number_of_nodes()\n",
    "    # Get the number of edges\n",
    "        DGES = G.number_of_edges()       \n",
    "        for _ in nL: \n",
    "        #for noiseL in nL: \n",
    "            for ptun in range(len(tun)): \n",
    "                folder = f'./{folderall}/{foldernames[k]}'\n",
    "                os.makedirs(f'{experimental_folder}{foldernames[k]}/{ptun}', exist_ok=True)\n",
    "                folder1=f'./{experimental_folder}/{foldernames[k]}/{ptun}'\n",
    "                file_A_results = open(f'{folder1}/Thesis_results.txt', 'w')\n",
    "                file_A_results.write(f'DGS DGES QGS QGES PGS PGES forb_norm accuracy spec_norm time isomorphic \\n')\n",
    "                F2 = np.loadtxt(f'./Data/data/{foldernames[k]}/{foldernames[k]}_t_feat.txt', dtype=float)  # shape: (n1, k)\n",
    "                F1 = np.loadtxt(f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_feat.txt', dtype=float)  # shape: (n2, k)\n",
    "                #diffs = B_feat[:, None, :] - A_feat[None, :, :]   # shape: (n1, n2, k)\n",
    "                #X = np.abs(diffs).sum(axis=2)  # shape: (n1, n2)\n",
    "                #Feat = np.linalg.norm(diffs, axis=2)  # shape: (n1, n2)\n",
    "                file_real_spectrum = open(f'{folder1}/real_Tspectrum{tuns[ptun]}.txt', 'w')\n",
    "                file_A_spectrum = open(f'{folder1}/A_Tspectrum{tuns[ptun]}.txt', 'w')\n",
    "                F2=F2\n",
    "                F1=F1\n",
    "                \n",
    "\n",
    "# Split into two arrays\n",
    "                if (foldernames[k]==\"douban\"):\n",
    "                    csv2 = pd.read_csv(f\"./Data/Full-dataset/attribute/{foldernames[k]}attr1.csv\", header=None).iloc[:, 1:].to_numpy()\n",
    "                    csv1 = pd.read_csv(f\"./Data/Full-dataset/attribute/{foldernames[k]}attr2.csv\", header=None).iloc[:, 1:].to_numpy()\n",
    "                \n",
    "\n",
    "                for iter in range(iters):\n",
    "                    if (foldernames[k]==\"douban\"):\n",
    "                        F2=csv2\n",
    "                        F1=csv1\n",
    "                    #you have to do that because the features have ID making them \n",
    "                    #giving ground truth information\n",
    "                    if (foldernames[k]==\"fb_tw\"):\n",
    "                        F2=F2*0\n",
    "                        F1=F1*0\n",
    "                        \n",
    "                    data_GT = np.loadtxt(f\"./Data/data/{foldernames[k]}/{foldernames[k]}_ground_True_0.1_{iter}.txt\", dtype=int)\n",
    "                    #F2=csv2\n",
    "                    #F1=csv1\n",
    "                    folder_ = f'{folder}/{iter}'\n",
    "                    folder1_ = f'{folder1}/{iter}'\n",
    "                    os.makedirs(f'{folder1_}', exist_ok=True)\n",
    "                    file_subgraph = f'{folder_}/subgraph.txt'\n",
    "                    file_nodes = f'{folder_}/nodes.txt'\n",
    "                    #Q_real = read_list(file_nodes)\n",
    "                    #G_Q= read_real_graph(n = n_Q, name_ = file_subgraph)\n",
    "                    G_Q = read_real_graph(n = n_G2[k], name_ = f'./Data/data/{foldernames[k]}/{foldernames[k]}_s_edge.txt')\n",
    "                    pairs = []\n",
    "                    with open(f'./Data/data/{foldernames[k]}/{foldernames[k]}_ground_True.txt', \"r\") as f:\n",
    "                        for line in f:\n",
    "                            a, b = line.strip().split()\n",
    "                            pairs.append((int(a), int(b)))\n",
    "\n",
    "# ✅ Find the max node ID in each graph\n",
    "                    max_A = n_G2[k]#max(a for a, b in pairs)\n",
    "                    max_B = n_G2[k]#max(b for a, b in pairs)\n",
    "\n",
    "# 2️⃣ Build A→B mapping with -1 for missing\n",
    "                    true1=False\n",
    "                    true2=False\n",
    "                    #if douban/dblp/fb_tw no+1 -allmv_tmdbwith +1\n",
    "                    if (foldernames[k]==\"allmv_tmdb\"):\n",
    "                        max_A=max_A+1\n",
    "                        max_B=max_B+1\n",
    "                    A_to_B = [-1] * (max_A)\n",
    "                    for a, b in pairs:\n",
    "                        if (a>=max_A):\n",
    "                            true1=True\n",
    "                        else:\n",
    "                         A_to_B[a] = b\n",
    "\n",
    "# 3️⃣ Build B→A mapping with -1 for missing\n",
    "                    B_to_A = [-1] * (max_B)\n",
    "                    for a, b in pairs:\n",
    "                        if (b>=max_B):\n",
    "                            true2=True\n",
    "                        else:\n",
    "                            B_to_A[b] = a\n",
    "                    print(true1,true2)\n",
    "                    #A = nx.adjacency_matrix(G_Q).todense()\n",
    "                    QGS=G_Q.number_of_nodes()\n",
    "                    QGES = G_Q.number_of_edges()\n",
    "                    anchors_G = data_GT[:, 1].tolist()\n",
    "                    anchors_GQ = data_GT[:, 0].tolist()\n",
    "                    #G1,G1_Q=synchronize_graphs(G,G_Q,anchors_G,anchors_GQ)\n",
    "                    print(max(anchors_G),\" value\")\n",
    "                    print(max(anchors_GQ),\" value\")\n",
    "                    print(np.shape(F2))\n",
    "                    print(\"G_Q\",G_Q.number_of_edges())\n",
    "                    print(\"G\",G.number_of_edges())\n",
    "                    if (foldernames[k]==\"douban\" or foldernames[k]==\"allmv_tmdb\"):\n",
    "                        anchors_G = data_GT[:, 0].tolist()\n",
    "                        anchors_GQ = data_GT[:, 1].tolist()\n",
    "                    \n",
    "                    G1,G1_Q=seed_link(anchors_G,anchors_GQ,G,G_Q)\n",
    "                    F2_n,F1_n=synchronize_features(F2,F1,anchors_G,anchors_GQ)\n",
    "                    #F2_n*=0\n",
    "                    #F1_n*=0\n",
    "                    start = time.time()\n",
    "                    #compare_features(F1,F2,A_to_B)\n",
    "                    #compare_features(F1,F2,B_to_A)\n",
    "                    if(tun[ptun]==1):\n",
    "                        print(\"Alpine\")\n",
    "                        mun=0.1\n",
    "                        if(foldernames[k]==\"fb_tw\"):\n",
    "                            mun=1\n",
    "                        _, list_of_nodes, forb_norm = Alpine_supervised(G1_Q.copy(), G1.copy(),F1_n,F2_n,anchors_GQ,anchors_G,mun,weight=2)                    \n",
    "                    elif(tun[ptun]==10):\n",
    "                        print(\"GradAlignP\")\n",
    "                        list_of_nodes, forb_norm = gradPMain(G_Q.copy(), G.copy(),F1.copy(),F2.copy(),anchors_GQ=anchors_GQ,anchors_G=anchors_G)\n",
    "                    else:\n",
    "                        print(\"Error\")\n",
    "                        exit()\n",
    "                    end = time.time()\n",
    "                    subgraph = G.subgraph(list_of_nodes)\n",
    "                    PGS=subgraph.number_of_nodes()\n",
    "                    PGES = subgraph.number_of_edges()\n",
    "                    isomorphic=False\n",
    "                    if(forb_norm==0):\n",
    "                        isomorphic=True\n",
    "                    time_diff = end - start\n",
    "                    file_nodes_pred = open(f'{folder1_}/{tuns[ptun]}.txt','w')\n",
    "                    for node in list_of_nodes: file_nodes_pred.write(f'{node}\\n')\n",
    "                    spec_norm=0\n",
    "                    #accuracy = np.sum(np.array(Q_real)==np.array(list_of_nodes))/len(Q_real)\n",
    "                    accuracy1 = np.sum(np.array(A_to_B)==np.array(list_of_nodes))/gt_size[0]\n",
    "                    accuracy2 = np.sum(np.array(B_to_A)==np.array(list_of_nodes))/gt_size[0]\n",
    "                    accuracy=0\n",
    "                    if ({foldernames[k]}==\"douban\" or{foldernames[k]}==\"allmv_tmdb\" ):\n",
    "                        accuracy=accuracy2\n",
    "                    else:\n",
    "                        accuracy=accuracy1\n",
    "                    print(\"ACC 1 or 2?\",accuracy1,accuracy2)\n",
    "                    print(np.sum(np.array(A_to_B)==np.array(list_of_nodes)))\n",
    "                    with open(\"differences.txt\", \"w\") as f:\n",
    "                        f.write(\"Differences A_to_B:\\n\")\n",
    "                        f.write(\"\\n\\nDifferences B_to_A:\\n\")\n",
    "                        f.write(\"\\n\\nAccuracy A_to_B: {:.4f}\\n\".format(accuracy1))\n",
    "                        f.write(\"Accuracy B_to_A: {:.4f}\\n\".format(accuracy2))\n",
    "                    file_A_results.write(f'{DGS} {DGES} {QGS} {QGES} {PGS} {PGES} {forb_norm} {accuracy1} {accuracy2} {time_diff} {isomorphic}\\n')\n",
    "                    printR(tuns[ptun],forb_norm,accuracy,spec_norm,time_diff,isomorphic)          \n",
    "            print('\\n')\n",
    "        print('\\n\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mypython)",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
